{
  "nodes": [
    {
      "id": "groqChat_0",
      "position": {
        "x": 24.266034135200485,
        "y": 24.433364389388316
      },
      "type": "customNode",
      "data": {
        "id": "groqChat_0",
        "label": "GroqChat",
        "version": 4,
        "name": "groqChat",
        "type": "GroqChat",
        "baseClasses": [
          "GroqChat",
          "BaseChatModel",
          "BaseLanguageModel",
          "Runnable"
        ],
        "category": "Chat Models",
        "description": "Wrapper around Groq API with LPU Inference Engine",
        "inputParams": [
          {
            "label": "Connect Credential",
            "name": "credential",
            "type": "credential",
            "credentialNames": [
              "groqApi"
            ],
            "optional": true,
            "id": "groqChat_0-input-credential-credential",
            "display": true
          },
          {
            "label": "Model Name",
            "name": "modelName",
            "type": "asyncOptions",
            "loadMethod": "listModels",
            "placeholder": "llama3-70b-8192",
            "id": "groqChat_0-input-modelName-asyncOptions",
            "display": true
          },
          {
            "label": "Temperature",
            "name": "temperature",
            "type": "number",
            "step": 0.1,
            "default": 0.9,
            "optional": true,
            "id": "groqChat_0-input-temperature-number",
            "display": true
          },
          {
            "label": "Max Tokens",
            "name": "maxTokens",
            "type": "number",
            "step": 1,
            "optional": true,
            "additionalParams": true,
            "id": "groqChat_0-input-maxTokens-number",
            "display": true
          },
          {
            "label": "Streaming",
            "name": "streaming",
            "type": "boolean",
            "default": true,
            "optional": true,
            "id": "groqChat_0-input-streaming-boolean",
            "display": true
          }
        ],
        "inputAnchors": [
          {
            "label": "Cache",
            "name": "cache",
            "type": "BaseCache",
            "optional": true,
            "id": "groqChat_0-input-cache-BaseCache",
            "display": true
          }
        ],
        "inputs": {
          "cache": "",
          "modelName": "llama-3.1-8b-instant",
          "temperature": "0.4",
          "maxTokens": "",
          "streaming": true
        },
        "outputAnchors": [
          {
            "id": "groqChat_0-output-groqChat-GroqChat|BaseChatModel|BaseLanguageModel|Runnable",
            "name": "groqChat",
            "label": "GroqChat",
            "description": "Wrapper around Groq API with LPU Inference Engine",
            "type": "GroqChat | BaseChatModel | BaseLanguageModel | Runnable"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 676,
      "selected": false,
      "positionAbsolute": {
        "x": 24.266034135200485,
        "y": 24.433364389388316
      },
      "dragging": false
    },
    {
      "id": "documentStoreVS_0",
      "position": {
        "x": 480.24958152436125,
        "y": 22.425027092628625
      },
      "type": "customNode",
      "data": {
        "id": "documentStoreVS_0",
        "label": "Document Store (Vector)",
        "version": 1,
        "name": "documentStoreVS",
        "type": "DocumentStoreVS",
        "baseClasses": [
          "DocumentStoreVS"
        ],
        "category": "Vector Stores",
        "description": "Search and retrieve documents from Document Store",
        "inputParams": [
          {
            "label": "Select Store",
            "name": "selectedStore",
            "type": "asyncOptions",
            "loadMethod": "listStores",
            "id": "documentStoreVS_0-input-selectedStore-asyncOptions",
            "display": true
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "selectedStore": "508a668c-2976-457b-80ca-c124fc211517"
        },
        "outputAnchors": [
          {
            "name": "output",
            "label": "Output",
            "type": "options",
            "description": "",
            "options": [
              {
                "id": "documentStoreVS_0-output-retriever-BaseRetriever",
                "name": "retriever",
                "label": "Retriever",
                "description": "",
                "type": "BaseRetriever"
              },
              {
                "id": "documentStoreVS_0-output-vectorStore-VectorStore",
                "name": "vectorStore",
                "label": "Vector Store",
                "description": "",
                "type": "VectorStore"
              }
            ],
            "default": "retriever"
          }
        ],
        "outputs": {
          "output": "retriever"
        },
        "selected": false
      },
      "width": 300,
      "height": 318,
      "selected": false,
      "dragging": false,
      "positionAbsolute": {
        "x": 480.24958152436125,
        "y": 22.425027092628625
      }
    },
    {
      "id": "conversationalRetrievalQAChain_0",
      "position": {
        "x": 1386.5617137967217,
        "y": 276.40800955998066
      },
      "type": "customNode",
      "data": {
        "id": "conversationalRetrievalQAChain_0",
        "label": "Conversational Retrieval QA Chain",
        "version": 3,
        "name": "conversationalRetrievalQAChain",
        "type": "ConversationalRetrievalQAChain",
        "baseClasses": [
          "ConversationalRetrievalQAChain",
          "BaseChain",
          "Runnable"
        ],
        "category": "Chains",
        "description": "Document QA - built on RetrievalQAChain to provide a chat history component",
        "inputParams": [
          {
            "label": "Return Source Documents",
            "name": "returnSourceDocuments",
            "type": "boolean",
            "optional": true,
            "id": "conversationalRetrievalQAChain_0-input-returnSourceDocuments-boolean",
            "display": true
          },
          {
            "label": "Rephrase Prompt",
            "name": "rephrasePrompt",
            "type": "string",
            "description": "Using previous chat history, rephrase question into a standalone question",
            "warning": "Prompt must include input variables: {chat_history} and {question}",
            "rows": 4,
            "additionalParams": true,
            "optional": true,
            "default": "Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\n\nChat History:\n{chat_history}\nFollow Up Input: {question}\nStandalone Question:",
            "id": "conversationalRetrievalQAChain_0-input-rephrasePrompt-string",
            "display": true
          },
          {
            "label": "Response Prompt",
            "name": "responsePrompt",
            "type": "string",
            "description": "Taking the rephrased question, search for answer from the provided context",
            "warning": "Prompt must include input variable: {context}",
            "rows": 4,
            "additionalParams": true,
            "optional": true,
            "default": "I want you to act as a document that I am having a conversation with. Your name is \"AI Assistant\". Using the provided context, answer the user's question to the best of your ability using the resources provided.\nIf there is nothing in the context relevant to the question at hand, just say \"Hmm, I'm not sure\" and stop after that. Refuse to answer any question not about the info. Never break character.\n------------\n{context}\n------------\nREMEMBER: If there is no relevant information within the context, just say \"Hmm, I'm not sure\". Don't try to make up an answer. Never break character.",
            "id": "conversationalRetrievalQAChain_0-input-responsePrompt-string",
            "display": true
          }
        ],
        "inputAnchors": [
          {
            "label": "Chat Model",
            "name": "model",
            "type": "BaseChatModel",
            "id": "conversationalRetrievalQAChain_0-input-model-BaseChatModel",
            "display": true
          },
          {
            "label": "Vector Store Retriever",
            "name": "vectorStoreRetriever",
            "type": "BaseRetriever",
            "id": "conversationalRetrievalQAChain_0-input-vectorStoreRetriever-BaseRetriever",
            "display": true
          },
          {
            "label": "Memory",
            "name": "memory",
            "type": "BaseMemory",
            "optional": true,
            "description": "If left empty, a default BufferMemory will be used",
            "id": "conversationalRetrievalQAChain_0-input-memory-BaseMemory",
            "display": true
          },
          {
            "label": "Input Moderation",
            "description": "Detect text that could generate harmful output and prevent it from being sent to the language model",
            "name": "inputModeration",
            "type": "Moderation",
            "optional": true,
            "list": true,
            "id": "conversationalRetrievalQAChain_0-input-inputModeration-Moderation",
            "display": true
          }
        ],
        "inputs": {
          "model": "{{groqChat_0.data.instance}}",
          "vectorStoreRetriever": "{{documentStoreVS_0.data.instance}}",
          "memory": "{{bufferWindowMemory_0.data.instance}}",
          "returnSourceDocuments": false,
          "rephrasePrompt": "Rewrite the user question into a clear, standalone question.\n\nRules:\n- Always output ONE rewritten question.\n- Do not leave the response empty.\n- Do not add explanations, labels, or extra text.\n- Preserve the original meaning exactly.\n- If the question is vague, rewrite it to include the relevant subject (IPage Vision or its services).\n\nChat History:\n{chat_history}\n\nUser Question:\n{question}\n\nOutput:\n",
          "responsePrompt": "You are an AI assistant for iPage Vision.\n\nAnswer the user’s question clearly and professionally using ONLY the information provided below.\nDo not mention the source of the information.\nDo not say phrases like “according to the provided information” or “based on the content”.\nDo not use external knowledge.\nDo not make assumptions.\nDo not fabricate or guess information.\n\nIf the information is fully available, provide a direct and confident answer.\nIf the information is partially available, summarize what is available clearly.\nIf the information is not available at all, respond EXACTLY with:\n\"I'm unable to find relevant information for that request.\"\n\nContent:\n------------\n{context}\n------------\n\nAnswer:\n",
          "inputModeration": ""
        },
        "outputAnchors": [
          {
            "id": "conversationalRetrievalQAChain_0-output-conversationalRetrievalQAChain-ConversationalRetrievalQAChain|BaseChain|Runnable",
            "name": "conversationalRetrievalQAChain",
            "label": "ConversationalRetrievalQAChain",
            "description": "Document QA - built on RetrievalQAChain to provide a chat history component",
            "type": "ConversationalRetrievalQAChain | BaseChain | Runnable"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 538,
      "positionAbsolute": {
        "x": 1386.5617137967217,
        "y": 276.40800955998066
      },
      "selected": false,
      "dragging": false
    },
    {
      "id": "bufferWindowMemory_0",
      "position": {
        "x": -49.03017159780953,
        "y": 868.056042155849
      },
      "type": "customNode",
      "data": {
        "id": "bufferWindowMemory_0",
        "label": "Buffer Window Memory",
        "version": 2,
        "name": "bufferWindowMemory",
        "type": "BufferWindowMemory",
        "baseClasses": [
          "BufferWindowMemory",
          "BaseChatMemory",
          "BaseMemory"
        ],
        "category": "Memory",
        "description": "Uses a window of size k to surface the last k back-and-forth to use as memory",
        "inputParams": [
          {
            "label": "Size",
            "name": "k",
            "type": "number",
            "default": "4",
            "description": "Window of size k to surface the last k back-and-forth to use as memory.",
            "id": "bufferWindowMemory_0-input-k-number",
            "display": true
          },
          {
            "label": "Session Id",
            "name": "sessionId",
            "type": "string",
            "description": "If not specified, a random id will be used. Learn <a target=\"_blank\" href=\"https://docs.flowiseai.com/memory#ui-and-embedded-chat\">more</a>",
            "default": "",
            "optional": true,
            "additionalParams": true,
            "id": "bufferWindowMemory_0-input-sessionId-string",
            "display": true
          },
          {
            "label": "Memory Key",
            "name": "memoryKey",
            "type": "string",
            "default": "chat_history",
            "additionalParams": true,
            "id": "bufferWindowMemory_0-input-memoryKey-string",
            "display": true
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "k": "40",
          "sessionId": "",
          "memoryKey": "chat_history"
        },
        "outputAnchors": [
          {
            "id": "bufferWindowMemory_0-output-bufferWindowMemory-BufferWindowMemory|BaseChatMemory|BaseMemory",
            "name": "bufferWindowMemory",
            "label": "BufferWindowMemory",
            "description": "Uses a window of size k to surface the last k back-and-forth to use as memory",
            "type": "BufferWindowMemory | BaseChatMemory | BaseMemory"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 337,
      "selected": false,
      "positionAbsolute": {
        "x": -49.03017159780953,
        "y": 868.056042155849
      },
      "dragging": false
    }
  ],
  "edges": [
    {
      "source": "groqChat_0",
      "sourceHandle": "groqChat_0-output-groqChat-GroqChat|BaseChatModel|BaseLanguageModel|Runnable",
      "target": "conversationalRetrievalQAChain_0",
      "targetHandle": "conversationalRetrievalQAChain_0-input-model-BaseChatModel",
      "type": "buttonedge",
      "id": "groqChat_0-groqChat_0-output-groqChat-GroqChat|BaseChatModel|BaseLanguageModel|Runnable-conversationalRetrievalQAChain_0-conversationalRetrievalQAChain_0-input-model-BaseChatModel"
    },
    {
      "source": "documentStoreVS_0",
      "sourceHandle": "documentStoreVS_0-output-retriever-BaseRetriever",
      "target": "conversationalRetrievalQAChain_0",
      "targetHandle": "conversationalRetrievalQAChain_0-input-vectorStoreRetriever-BaseRetriever",
      "type": "buttonedge",
      "id": "documentStoreVS_0-documentStoreVS_0-output-retriever-BaseRetriever-conversationalRetrievalQAChain_0-conversationalRetrievalQAChain_0-input-vectorStoreRetriever-BaseRetriever"
    },
    {
      "source": "bufferWindowMemory_0",
      "sourceHandle": "bufferWindowMemory_0-output-bufferWindowMemory-BufferWindowMemory|BaseChatMemory|BaseMemory",
      "target": "conversationalRetrievalQAChain_0",
      "targetHandle": "conversationalRetrievalQAChain_0-input-memory-BaseMemory",
      "type": "buttonedge",
      "id": "bufferWindowMemory_0-bufferWindowMemory_0-output-bufferWindowMemory-BufferWindowMemory|BaseChatMemory|BaseMemory-conversationalRetrievalQAChain_0-conversationalRetrievalQAChain_0-input-memory-BaseMemory"
    }
  ]
}